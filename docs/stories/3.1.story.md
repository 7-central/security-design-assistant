# Story 3.1: Serverless Queue Implementation

## Status
Done

## Story
**As a** system,  
**I want** to process drawing requests asynchronously through SQS and Lambda,  
**so that** the system automatically scales with demand and costs scale with usage.

## Acceptance Criteria
1. Create SQS queue with visibility timeout of 30 minutes (max Lambda duration)
2. Modify `/process-drawing` endpoint (Lambda function) to:
   - Generate job ID and save initial status to DynamoDB
   - Send message to SQS with job details
   - Return: `{"job_id": "job_<timestamp>", "status": "queued"}`
3. Create processor Lambda function triggered by SQS:
   - Batch size: 1 (one drawing per invocation)
   - Timeout: 15 minutes (Lambda max)
   - Memory: 3GB (for PDF processing)
4. DynamoDB table for job tracking:
   - Partition key: company#client#job (CORRECTED from epic AC 4)
   - Attributes: status, created_at, updated_at, result, error
   - TTL: 30 days for automatic cleanup
5. Implement `/status/{job_id}` endpoint (separate Lambda) reading from DynamoDB
6. Dead Letter Queue (DLQ) for failed messages after 3 attempts
7. CloudWatch alarms for DLQ depth > 5 messages

## Tasks / Subtasks

- [x] **Create SQS Infrastructure (AC: 1, 6)** *DEPENDS ON: None*
  - [x] Create main processing queue with 30-minute visibility timeout
  - [x] Create Dead Letter Queue (DLQ) for failed messages
  - [x] Configure automatic retries (3 attempts with exponential backoff)
  - [x] Set up queue permissions for Lambda functions

- [x] **Create DynamoDB Job Tracking Table (AC: 4)** *DEPENDS ON: None*
  - [x] Create `security-assistant-jobs` table with partition key `company#client#job`
  - [x] Add GSI1: StatusDateIndex (status, created_at)
  - [x] Add GSI2: ClientProjectIndex (client_name, created_at) 
  - [x] Add GSI3: DateRangeIndex (date_bucket, created_at)
  - [x] Configure TTL for 30-day automatic cleanup
  - [x] Set up table permissions for Lambda functions

- [x] **Update Storage Interface for Serverless (AC: 2, 3)** *DEPENDS ON: DynamoDB table creation*
  - [x] Extend `aws_storage.py` to include DynamoDB job status operations
  - [x] Add job status save/load methods to `interface.py`
  - [x] Update checkpoint saving to include job status updates
  - [x] Ensure presigned URL generation for file downloads
  - [x] Test storage abstraction with Lambda environment

- [x] **Modify API Gateway Endpoint (AC: 2)** *DEPENDS ON: SQS queue, DynamoDB table, Storage interface updates*
  - [x] Update `/process-drawing` Lambda function to async processing model
  - [x] Generate job ID using existing `id_generator.py` utilities
  - [x] Save initial job record to DynamoDB with "queued" status
  - [x] Send message to SQS queue with job details and file paths
  - [x] Return immediate response with job_id and status
  - [x] Update response schema to match REST API spec
  - [x] Add Gemini rate limit handling with exponential backoff
  - [x] Implement Lambda timeout detection and graceful handling

- [x] **Create Processing Lambda Function (AC: 3)** *DEPENDS ON: SQS queue, DynamoDB table, Storage interface updates*
  - [x] Create new Lambda function `process-drawing-worker`
  - [x] Configure SQS trigger with batch size 1
  - [x] Set timeout to 15 minutes and memory to 3GB
  - [x] Integrate existing processing orchestrator from `src/api/main.py`
  - [x] Update job status to "processing" at start
  - [x] Handle checkpoint saves throughout processing with status updates
  - [x] Update final job status on completion/failure
  - [x] Implement graceful Lambda timeout handling with progress save
  - [x] Add retry logic for transient Gemini API failures

- [x] **Create Status Check Lambda Function (AC: 5)** *DEPENDS ON: DynamoDB table*
  - [x] Create new Lambda function `get-job-status`
  - [x] Implement `/status/{job_id}` endpoint handler
  - [x] Query DynamoDB for job status and progress information
  - [x] Return structured response per REST API spec
  - [x] Include download URLs for completed files
  - [x] Handle job not found scenarios

- [x] **Configure CloudWatch Monitoring (AC: 7)** *DEPENDS ON: SQS queues created*
  - [x] Create CloudWatch alarm for DLQ message depth > 5
  - [x] Set up SNS topic for critical alerts
  - [x] Configure alarm actions for DLQ overflow
  - [x] Add Lambda error rate monitoring
  - [x] Set up SQS message age monitoring

- [x] **Create Unit Tests for New Components (AC: All)** *DEPENDS ON: All implementation tasks completed*
  - [x] Create `tests/unit/test_lambda/test_process_drawing_api.py`
  - [x] Create `tests/unit/test_lambda/test_process_drawing_worker.py` 
  - [x] Create `tests/unit/test_lambda/test_get_job_status.py`
  - [x] Create `tests/unit/test_storage/test_dynamodb_operations.py`
  - [x] Mock SQS and DynamoDB operations for consistent testing
  - [x] Test error scenarios and edge cases including Gemini rate limits
  - [x] Test Lambda timeout scenarios and graceful degradation
  - [x] Follow existing test patterns from `tests/unit/test_storage/`

## Dev Notes

### Previous Story Insights
[Source: Story 2.5 Completion Notes]
- Full pipeline processing works: context → schedule → excel_generation → judge
- Pipeline selection via `PIPELINE_CONFIGS['full_analysis']` and `PIPELINE_CONFIGS['no_context']`
- Storage abstraction supports both local and AWS modes via `STORAGE_MODE` environment variable
- Existing job processing orchestrator in `src/api/main.py` handles full pipeline coordination
- Checkpoint system saves intermediate state after each agent completes
- All agents use Google GenAI SDK (0.2.0) with Gemini models

### Architecture Context

**Current API Structure**
[Source: architecture/components.md#api-service]
- FastAPI application in `src/api/main.py` with `/process-drawing` endpoint
- Current implementation: synchronous processing returning Excel file directly
- Uses `ProcessingOrchestrator` to coordinate multi-agent pipeline
- Checkpoint saves after each stage: context, components, excel_generation, judge

**Target Serverless Architecture**  
[Source: architecture/infrastructure-and-deployment.md]
- AWS Lambda functions with API Gateway frontend
- SQS for async job queuing and processing
- DynamoDB for job status and metadata storage
- S3 for file storage (already implemented via storage abstraction)

**Job Data Model**
[Source: architecture/database-schema.md#jobs-table]
- **CORRECTED** Partition key: `company#client#job` (e.g., "7central#st_marys#job_123")
- Status tracking: queued, processing, completed, failed
- TTL: 30 days for automatic cleanup
- Checkpoint system for recovery and progress tracking

**Database Schema Details**
[Source: architecture/database-schema.md#jobs-table]
```
Table Name: security-assistant-jobs
Partition Key: company#client#job (String)
Attributes: company_id, client_name, project_name, job_id, status, created_at, updated_at, input_files, checkpoints, output_files, metadata, error, ttl
```

**Required GSI Indexes:**
- GSI1: StatusDateIndex (status, created_at) - Query jobs by status
- GSI2: ClientProjectIndex (client_name, created_at) - Query jobs by client  
- GSI3: DateRangeIndex (date_bucket, created_at) - Query jobs by date range

**Lambda Function Architecture**
[Source: architecture/components.md#processing-orchestrator]
- `process-drawing-api`: Handles uploads, creates jobs, queues processing
- `process-drawing-worker`: SQS-triggered processing with existing orchestrator
- `get-job-status`: Status endpoint reading from DynamoDB

**File Storage Structure**
[Source: architecture/database-schema.md#s3-bucket-structure]
```
security-assistant-files/
├── 7central/
│   ├── client_name/
│   │   ├── project_name/
│   │   │   ├── job_id/
│   │   │   │   ├── drawing.pdf
│   │   │   │   ├── context.docx (optional)
│   │   │   │   ├── checkpoint_*.json
│   │   │   │   ├── schedule_v1.xlsx
│   │   │   │   └── evaluation.json
```

**API Response Schemas**
[Source: architecture/rest-api-spec.md]
- `/process-drawing` returns: `{"job_id": "job_1234567890", "status": "queued", "estimated_time_seconds": 300}`
- `/status/{job_id}` returns: Job status with progress, files, summary, and error details
- `/download/{job_id}/{file_type}` redirects to S3 presigned URLs

**Error Handling Strategy**
[Source: Epic 3.1 AC 6, 7]
- SQS automatic retries: 3 attempts with exponential backoff
- Dead Letter Queue for failed messages after max retries
- CloudWatch alarms for DLQ depth > 5 messages
- Lambda timeout handling: Save progress to DynamoDB before timeout
- **NEW**: Gemini API rate limit handling with exponential backoff retry
- **NEW**: Graceful Lambda timeout detection and progress preservation

### Technical Implementation Details

**Technology Stack**
[Source: architecture/tech-stack.md]
- Python 3.11 for all Lambda functions
- AWS Lambda runtime with boto3 for AWS services
- Google GenAI SDK 0.2.0 for AI processing
- Gemini 2.5 Flash for cost-effective processing
- FastAPI 0.109.0 for API framework
- pytest 8.0.0 for testing

**SQS Message Format**
```json
{
  "job_id": "job_1734567890",
  "company_client_job": "7central#st_marys#job_1734567890",
  "drawing_s3_key": "7central/st_marys/project/job_1734567890/drawing.pdf",
  "context_s3_key": "7central/st_marys/project/job_1734567890/context.docx",
  "pipeline_config": "full_analysis",
  "client_name": "St. Mary's Hospital",
  "project_name": "Emergency Wing Expansion"
}
```

**Lambda Environment Variables**
```bash
STORAGE_MODE=aws
GEMINI_API_KEY=<from_parameter_store>
S3_BUCKET=security-assistant-files
DYNAMODB_TABLE=security-assistant-jobs
SQS_QUEUE_URL=<queue_url>
```

**DynamoDB Item Structure**
```json
{
  "company#client#job": "7central#st_marys#job_1734567890",
  "job_id": "job_1734567890", 
  "status": "processing",
  "created_at": "2025-08-10T10:00:00Z",
  "updated_at": "2025-08-10T10:05:32Z",
  "stages_completed": ["context", "schedule"],
  "current_stage": "excel_generation",
  "input_files": {
    "drawing": "s3_key_path",
    "context": "s3_key_path"
  },
  "output_files": {},
  "metadata": {
    "client_name": "St. Mary's Hospital",
    "project_name": "Emergency Wing Expansion",
    "file_name": "drawing.pdf"
  },
  "ttl": 1739751890
}
```

**Integration Points with Existing Code**
- Use existing `StorageInterface` from `src/storage/interface.py`
- Extend `aws_storage.py` with DynamoDB job operations
- Reuse `ProcessingOrchestrator` logic from `src/api/main.py`
- Maintain existing agent interfaces from `src/agents/`
- Use existing job ID generation from `src/utils/id_generator.py`

### File Structure for New Components
[Source: architecture/source-tree.md]
```
src/
├── lambda/                     (NEW)
│   ├── __init__.py
│   ├── process_drawing_api.py  # Upload endpoint Lambda
│   ├── process_drawing_worker.py # SQS processor Lambda  
│   └── get_job_status.py       # Status endpoint Lambda
├── storage/
│   ├── aws_storage.py          # EXTEND with DynamoDB ops
│   └── interface.py            # ADD job status methods
infrastructure/                 (NEW)
├── template.yaml               # SAM template
├── samconfig.toml             # SAM configuration  
└── buildspec.yml              # CodeBuild spec
tests/
├── unit/
│   ├── test_lambda/           (NEW)
│   │   ├── test_process_drawing_api.py
│   │   ├── test_process_drawing_worker.py
│   │   └── test_get_job_status.py
│   └── test_storage/
│       └── test_dynamodb_operations.py (NEW)
```

## Testing

### Testing Standards
[Source: architecture/test-strategy-and-standards.md]

**Test Organization:**
- Unit tests in `tests/unit/` mirroring source structure
- Integration tests in `tests/integration/`
- Evaluation tests (real API calls) in `tests/evaluation/`

**Testing Framework:**
- pytest 8.0.0
- Mock AWS services using moto library for S3/DynamoDB/SQS
- Mock Gemini API responses using unittest.mock
- Follow AAA pattern (Arrange, Act, Assert)
- Test naming: `test_<component>_<action>_<expected_outcome>`

**Lambda Testing Strategy:**
- Unit tests: Mock all AWS services, test business logic
- Integration tests: Test with moto-mocked AWS services
- Evaluation tests: Test against real AWS staging environment  

**Mock Patterns for AWS Services:**
```python
import boto3
from moto import mock_dynamodb, mock_sqs, mock_s3

@mock_dynamodb
@mock_sqs
def test_process_drawing_api_creates_job():
    # Setup mocked AWS resources
    dynamodb = boto3.resource('dynamodb', region_name='us-east-1')
    sqs = boto3.client('sqs', region_name='us-east-1')
    
    # Create test table and queue
    table = dynamodb.create_table(...)
    queue_url = sqs.create_queue(QueueName='test-queue')['QueueUrl']
    
    # Test Lambda function logic
    response = process_drawing_api_handler(test_event, {})
    
    # Verify DynamoDB record created
    assert table.get_item(Key={'company#client#job': 'test_job'})['Item']
    
    # Verify SQS message sent
    messages = sqs.receive_message(QueueUrl=queue_url)
    assert len(messages.get('Messages', [])) == 1
```

**Serverless Testing Requirements:**
- Test SQS message processing with different payload formats
- Test DynamoDB CRUD operations with proper error handling
- Test Lambda timeout scenarios and graceful degradation
- Test dead letter queue handling for failed messages
- Test CloudWatch alarm triggering conditions
- **NEW**: Test Gemini API rate limit handling and retry logic
- **NEW**: Test Lambda timeout detection and progress preservation

**Test Data Management:**
- Use test fixtures from `tests/fixtures/` for consistent data
- Mock AWS service responses for unit tests
- Use moto library for AWS service integration tests
- Stage environment for end-to-end serverless tests

## Dev Agent Record
[This section is populated by the development agent during implementation]

### Agent Model Used
Claude Sonnet 4 (claude-sonnet-4-20250514) - Full Stack Development Agent

### Debug Log References  
- Lambda function import testing verified successfully
- AWS storage layer integration confirmed
- Infrastructure template validation completed

### Completion Notes List
- **SQS Infrastructure**: Created complete SAM template with main processing queue, dead letter queue, CloudWatch alarms, and SNS alerts
- **DynamoDB Schema**: Implemented multi-tenant job tracking table with composite key structure and three Global Secondary Indexes
- **AWS Storage Layer**: Developed comprehensive AWSStorage class with S3 file operations, DynamoDB job status management, and presigned URL generation  
- **Lambda Functions**: Created three serverless functions:
  - `process_drawing_api`: Async job creation endpoint with file validation and SQS messaging
  - `process_drawing_worker`: Full pipeline processor handling PDF analysis, context, component extraction, Excel generation, and evaluation
  - `get_job_status`: Status endpoint with progress tracking, file downloads, and comprehensive response format
- **Testing Infrastructure**: Comprehensive unit test suite with mocked AWS services using moto library
- **Timeout Handling**: Implemented Lambda timeout detection and graceful degradation with progress preservation
- **Error Handling**: Added comprehensive error handling for PDF corruption, rate limits, and infrastructure failures
- **Directory Structure**: Renamed `src/lambda` to `src/lambda_functions` to avoid Python keyword conflicts

### File List
#### Infrastructure
- `infrastructure/template.yaml` - Complete SAM template with all AWS resources
- `infrastructure/samconfig.toml` - SAM deployment configuration for multiple environments  
- `infrastructure/buildspec.yml` - CodeBuild specification for CI/CD pipeline

#### Source Code
- `src/lambda_functions/__init__.py` - Lambda functions package
- `src/lambda_functions/process_drawing_api.py` - API Gateway endpoint for job creation
- `src/lambda_functions/process_drawing_worker.py` - SQS-triggered processing worker
- `src/lambda_functions/get_job_status.py` - Job status query endpoint
- `src/storage/aws_storage.py` - AWS S3 and DynamoDB storage implementation
- `src/storage/interface.py` - Extended storage interface with presigned URL support
- `src/storage/local_storage.py` - Updated local storage with new interface methods
- `src/config/settings.py` - Added AWS service configuration settings

#### Tests  
- `tests/unit/test_lambda/__init__.py` - Lambda tests package
- `tests/unit/test_lambda/test_process_drawing_api.py` - API endpoint tests
- `tests/unit/test_lambda/test_process_drawing_worker.py` - Worker function tests  
- `tests/unit/test_lambda/test_get_job_status.py` - Status endpoint tests
- `tests/unit/test_storage/test_dynamodb_operations.py` - DynamoDB operations tests

#### Configuration
- `requirements.txt` - Added AWS testing dependencies (boto3, moto)

## QA Results

### Review Date: 2025-08-10

### Reviewed By: Quinn (Senior Developer QA)

### Code Quality Assessment

**Overall Assessment: EXCELLENT** - This is a well-architected serverless implementation that properly follows AWS best practices and maintains clean separation of concerns. The code demonstrates strong understanding of serverless patterns, error handling, and scalability considerations.

**Architecture Strengths:**
- Proper multi-tenant data model with composite keys
- Clean storage abstraction pattern allowing local/AWS modes
- Comprehensive timeout detection and graceful degradation
- Well-structured Lambda functions with single responsibilities
- Proper async/await patterns with sync Lambda handler compatibility

### Refactoring Performed

- **File**: `src/lambda_functions/process_drawing_api.py`
  - **Change**: Moved AWS SQS client initialization from module level to handler function scope
  - **Why**: Module-level AWS client initialization prevents proper testing and can cause import errors in test environments
  - **How**: Improves testability by allowing proper mocking and prevents region configuration issues during testing

- **File**: `src/lambda_functions/process_drawing_worker.py`
  - **Change**: Removed unused module-level SQS client initialization
  - **Why**: The client was declared but never used, creating unnecessary import overhead
  - **How**: Reduces module initialization complexity and eliminates unused dependencies

### Compliance Check

- Coding Standards: ✓ **Excellent** - Clean Python code following PEP standards, proper type hints, comprehensive error handling
- Project Structure: ✓ **Good** - Logical file organization, proper separation of concerns, renamed `src/lambda` to avoid Python keyword conflicts  
- Testing Strategy: ✓ **Good** - Comprehensive unit tests with proper mocking patterns, though coding standards docs are missing
- All ACs Met: ✓ **Complete** - All 7 acceptance criteria fully implemented and validated

### Improvements Checklist

**Completed by QA:**
- [x] Fixed AWS client initialization pattern for better testability (`process_drawing_api.py`, `process_drawing_worker.py`)
- [x] Validated comprehensive error handling across all failure modes
- [x] Confirmed proper timeout detection and graceful degradation
- [x] Verified all infrastructure resources properly configured in SAM template

**No Additional Changes Required:**
- Architecture is sound and follows AWS Lambda best practices
- Error handling is comprehensive with proper logging
- Infrastructure template covers all required resources with proper configuration
- Storage abstraction properly supports both local and AWS modes

### Security Review

**EXCELLENT** - Implementation demonstrates strong security practices:
- S3 bucket properly configured with public access blocked
- DynamoDB permissions scoped to specific tables
- No secrets or keys committed to repository
- Proper CORS configuration for API endpoints
- TTL configured for automatic data cleanup
- Presigned URLs used for secure file downloads

### Performance Considerations

**VERY GOOD** - Well-optimized serverless architecture:
- Lambda memory allocation appropriate for workload (3GB for PDF processing)
- SQS batch size of 1 prevents memory issues with large PDFs
- Concurrent execution limits prevent resource exhaustion
- Proper timeout configuration prevents hanging processes
- CloudWatch monitoring configured for operational visibility

### Final Status

**✓ Approved - Ready for Done**

This implementation represents high-quality serverless architecture with proper error handling, monitoring, and scalability patterns. The minor refactoring performed improves testability without changing functionality. All acceptance criteria are met and the code is production-ready.

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-08-10 | 1.0 | Initial story creation based on Epic 3 requirements | Bob (Scrum Master) |
| 2025-08-10 | 1.1 | Corrected story addressing PO validation issues: Added missing template sections, resolved DynamoDB schema inconsistency, clarified task dependencies, enhanced error handling coverage | Bob (Scrum Master) |